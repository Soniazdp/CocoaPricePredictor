{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(df, df_name):\n",
    "    \"\"\"Validate and clean data, keeping first of duplicate pairs\"\"\"\n",
    "    print(f\"\\nValidating {df_name}...\")\n",
    "    \n",
    "    # Define duplicate criteria\n",
    "    if df_name == \"Ghana data\":\n",
    "        dup_cols = ['station', 'name', 'date']\n",
    "    elif df_name == \"ICCO data\":\n",
    "        dup_cols = ['date', 'price_usd_per_tonne']\n",
    "    else:\n",
    "        dup_cols = ['date']\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicates = df[df.duplicated(dup_cols, keep=False)]\n",
    "    \n",
    "    if not duplicates.empty:\n",
    "        print(\"Duplicate pairs:\", duplicates.sort_values(dup_cols))\n",
    "        \n",
    "        # Keep first of each pair\n",
    "        df = df.drop_duplicates(dup_cols, keep='first')\n",
    "        print(f\"Now duplicated rows: {len(df[df.duplicated(dup_cols)])}\")\n",
    "        # print(f\"Num of rows with 2023-12-15 and price 4272.15: {len(df[(df['date'] == '2023-12-15') & (df['price_usd_per_tonne' == 4272.15)])}\")\n",
    "        # print(f\"Num of rows with 2024-01-09 and price 4171.24: {len(df[(df['date'] == '2024-01-09') & (df['price_usd_per_tonne'] == 4171.24)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_anomalies(df):\n",
    "    \"\"\"Check for data anomalies\"\"\"\n",
    "    print(\"\\nRunning Data Quality Checks:\")\n",
    "    \n",
    "    # Temperature logical consistency\n",
    "    try:\n",
    "        assert (df['max_temp'] >= df['avg_temp']).all(), \"Max temp < Avg temp\"\n",
    "        assert (df['avg_temp'] >= df['min_temp']).all(), \"Avg temp < Min temp\"\n",
    "        print(\"✓ Temperature consistency checks passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Temperature anomaly: {e}\")\n",
    "    \n",
    "    # Precipitation non-negative\n",
    "    try:\n",
    "        assert (df['precipitation'] >= 0).all(), \"Negative precipitation\"\n",
    "        print(\"✓ Precipitation non-negativity check passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Precipitation anomaly: {e}\")\n",
    "    \n",
    "    # Price positive\n",
    "    try:\n",
    "        assert (df['price_usd_per_tonne'] > 0).all(), \"Non-positive prices\"\n",
    "        print(\"✓ Price positivity check passed\")\n",
    "    except AssertionError as e:\n",
    "        print(f\"Price anomaly: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and clean Ghana weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghana = pd.read_csv('Ghana_data.csv')\n",
    "ghana = ghana.rename(columns={\n",
    "    'STATION': 'station',\n",
    "    'NAME': 'name',\n",
    "    'DATE': 'date',\n",
    "    'PRCP': 'precipitation',\n",
    "    'TMAX': 'max_temp',\n",
    "    'TMIN': 'min_temp',\n",
    "    'TAVG': 'avg_temp',\n",
    "})\n",
    "ghana['date'] = pd.to_datetime(ghana['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and clean ICCO price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "icco_prices = pd.read_csv(\n",
    "    'Daily Prices_ICCO.csv',\n",
    "    thousands=',',\n",
    "    decimal='.',\n",
    "    parse_dates=['Date'],\n",
    "    dayfirst=True\n",
    ")\n",
    "icco_prices = icco_prices.rename(columns={\n",
    "    'Date': 'date',\n",
    "    'ICCO daily price (US$/tonne)': 'price_usd_per_tonne',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate data quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating Ghana data...\n",
      "\n",
      "Validating ICCO data...\n",
      "Duplicate pairs:           date  price_usd_per_tonne\n",
      "310 2023-12-15              4272.15\n",
      "311 2023-12-15              4272.15\n",
      "295 2024-01-09              4171.24\n",
      "296 2024-01-09              4171.24\n",
      "Now duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "validate_data(ghana, \"Ghana data\")\n",
    "validate_data(icco_prices, \"ICCO data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date coverage analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date Coverage Analysis:\n",
      "\n",
      "  Data Source Start Date   End Date  Unique Dates\n",
      "0       Ghana 1990-01-01 2024-11-28         10944\n",
      "1        ICCO 1994-10-03 2025-02-27          7808\n"
     ]
    }
   ],
   "source": [
    "date_coverage = pd.DataFrame({\n",
    "    'Data Source': ['Ghana', 'ICCO'],\n",
    "    'Start Date': [ghana['date'].min(), icco_prices['date'].min()],\n",
    "    'End Date': [ghana['date'].max(), icco_prices['date'].max()],\n",
    "    'Unique Dates': [ghana['date'].nunique(), icco_prices['date'].nunique()]\n",
    "})\n",
    "print(\"\\nDate Coverage Analysis:\")\n",
    "print(f\"\\n{date_coverage.to_string()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the datasets on date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre merge counts: {'Ghana': 53231, 'ICCO': 7812}\n",
      "Merged successfully. Kept 35318 possible records\n"
     ]
    }
   ],
   "source": [
    "pre_merge_counts = {\n",
    "    'Ghana': len(ghana),\n",
    "    'ICCO': len(icco_prices)\n",
    "}\n",
    "\n",
    "merged_data = pd.merge(\n",
    "    ghana,\n",
    "    icco_prices,\n",
    "    on='date',\n",
    "    how='inner',  # Only keep dates with both weather and price data\n",
    ")\n",
    "\n",
    "merged_data = merged_data.sort_values('date')\n",
    "print(f\"Pre merge counts: {pre_merge_counts}\")\n",
    "print(f\"Merged successfully. Kept {len(merged_data)} possible records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean station names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All names end with 'GH' - removing suffix\n"
     ]
    }
   ],
   "source": [
    "all_end_with_GH = merged_data['name'].str.endswith(\"GH\").all()\n",
    "if all_end_with_GH:\n",
    "    print(\"All names end with 'GH' - removing suffix\")\n",
    "    merged_data['name'] = merged_data['name'].str[:-4]\n",
    "else:\n",
    "    print(\"Not all names end with 'GH' - check naming consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['year'] = merged_data['date'].dt.year\n",
    "merged_data['month'] = merged_data['date'].dt.month\n",
    "merged_data['day'] = merged_data['date'].dt.day\n",
    "merged_data['day_of_year'] = merged_data['date'].dt.dayofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data[[\n",
    "    'station', 'name', 'date', 'year', 'month', 'day', 'day_of_year',\n",
    "    'precipitation', 'max_temp', 'min_temp', 'avg_temp',\n",
    "    'price_usd_per_tonne'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing(df):\n",
    "    \"\"\"Impute missing values with appropriate methods\"\"\"\n",
    "    print(\"\\nHandling missing values...\")\n",
    "    \n",
    "    # Show only columns with missing values\n",
    "    missing_before = df.isnull().sum()\n",
    "    missing_before = missing_before[missing_before > 0]\n",
    "    print(f\"Missing values before handling:\\n{missing_before}\")\n",
    "\n",
    "    # Precipitation: 0 for missing (assuming no rain)\n",
    "    df['precipitation'] = df['precipitation'].fillna(0)\n",
    "    \n",
    "    # Temperatures: linear interpolation for missing\n",
    "    for col in ['max_temp', 'min_temp', 'avg_temp']:\n",
    "        df[col] = df[col].interpolate(method='linear', limit_direction='both')\n",
    "        \n",
    "    # Show missing values after handling\n",
    "    missing_after = df.isnull().sum()\n",
    "    missing_after = missing_after[missing_after > 0]\n",
    "    print(f\"Missing values after handling:\\n{missing_after}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ghana missing values:\n",
      "precipitation    35442\n",
      "max_temp         18368\n",
      "min_temp         19018\n",
      "dtype: int64\n",
      "\n",
      "ICCO missing values:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Handling missing values...\n",
      "Missing values before handling:\n",
      "precipitation    23752\n",
      "max_temp         11869\n",
      "min_temp         12679\n",
      "dtype: int64\n",
      "Missing values after handling:\n",
      "Series([], dtype: int64)\n",
      "\n",
      "Running Data Quality Checks:\n",
      "Temperature anomaly: Max temp < Avg temp\n",
      "✓ Precipitation non-negativity check passed\n",
      "✓ Price positivity check passed\n"
     ]
    }
   ],
   "source": [
    "ghana_missing = ghana.isnull().sum()\n",
    "ghana_missing = ghana_missing[ghana_missing > 0]\n",
    "icco_missing = icco_prices.isnull().sum()\n",
    "icco_missing = icco_missing[icco_missing > 0]\n",
    "print(f\"\\nGhana missing values:\\n{ghana_missing}\")\n",
    "print(f\"\\nICCO missing values:\\n{icco_missing}\")\n",
    "\n",
    "merged_data = handle_missing(merged_data)\n",
    "check_anomalies(merged_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving daily merged data to daily_merged.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSaving daily merged data to daily_merged.csv\")\n",
    "merged_data.to_csv('daily_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price consistency check:\n",
      "1    359\n",
      "Name: monthly_price_mean, dtype: int64\n",
      "\n",
      "Date Range Verification:\n",
      "Train Full: 1994-10 to 2020-02 (2296 months)\n",
      "Test Full: 2020-02 to 2024-11 (575 months)\n",
      "Saved train_full.csv\n",
      "Saved test_full.csv\n",
      "Saved train_simple.csv\n",
      "Saved test_simple.csv\n",
      "\n",
      "All data processed and split successfully.\n"
     ]
    }
   ],
   "source": [
    "simplified_cols = [\n",
    "    'station', 'name', 'date', 'year', 'month',\n",
    "    'precipitation_sum',\n",
    "    'max_temp_mean',\n",
    "    'min_temp_mean',\n",
    "    'avg_temp_mean',\n",
    "    'monthly_price_mean',\n",
    "    'monthly_price_last',\n",
    "]\n",
    "\n",
    "# Step 1: Aggregate PRICES by month only (global prices)\n",
    "monthly_prices = (\n",
    "    merged_data.groupby(pd.Grouper(key='date', freq='M'))\n",
    "    ['price_usd_per_tonne'].agg(['mean', 'last', 'std'])\n",
    "    .reset_index()\n",
    "    .rename(columns={\n",
    "        'mean': 'monthly_price_mean',\n",
    "        'last': 'monthly_price_last',\n",
    "        'std': 'monthly_price_std'\n",
    "    })\n",
    ")\n",
    "\n",
    "# Step 2: Aggregate WEATHER by month+station\n",
    "agg_weather = (\n",
    "    merged_data.groupby([pd.Grouper(key='date', freq='M'), 'station', 'name'])\n",
    "    .agg({\n",
    "        'precipitation': ['sum', 'max', 'count'],\n",
    "        'max_temp': ['mean', 'max'],\n",
    "        'min_temp': ['mean', 'min'],\n",
    "        'avg_temp': ['mean', 'std']\n",
    "    })\n",
    "    .pipe(lambda df: df.set_axis(\n",
    "        ['_'.join(col).strip() for col in df.columns], \n",
    "        axis=1\n",
    "    ))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 3: Merge the monthly prices back into weather data\n",
    "agg_data = pd.merge(\n",
    "    agg_weather,\n",
    "    monthly_prices,\n",
    "    on='date',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Add year/month columns\n",
    "agg_data = agg_data.assign(\n",
    "    year=lambda x: x['date'].dt.year,\n",
    "    month=lambda x: x['date'].dt.month\n",
    ")\n",
    "\n",
    "# [['station', 'name', 'date', 'year', 'month',\n",
    "#   'precipitation_sum', 'precipitation_max', 'precipitation_count',\n",
    "#   'max_temp_mean', 'max_temp_max',\n",
    "#   'min_temp_mean', 'min_temp_min',\n",
    "#   'avg_temp_mean', 'avg_temp_std',\n",
    "#   'price_usd_per_tonne_mean', 'price_usd_per_tonne_last', 'price_usd_per_tonne_std']]\n",
    "\n",
    "# Handle NaN std values (now only for temperature)\n",
    "std_columns = [col for col in agg_data.columns if '_std' in col]\n",
    "for col in std_columns:\n",
    "    agg_data[col] = agg_data[col].fillna(0)\n",
    "\n",
    "# Verify prices are consistent per month\n",
    "print(\"Price consistency check:\")\n",
    "print(agg_data.groupby('date')['monthly_price_mean'].nunique().value_counts())\n",
    "# Should show all 1's (only 1 unique price per month)\n",
    "\n",
    "agg_data.to_csv('monthly_merged.csv', index=False)\n",
    "\n",
    "# ====== TIME-BASED SPLIT ======\n",
    "def time_series_split(df, test_ratio=0.2):\n",
    "    \"\"\"Split DataFrame chronologically into train and test sets\"\"\"\n",
    "    df = df.sort_values('date')  # Ensure chronological order\n",
    "    split_idx = int(len(df) * (1 - test_ratio))\n",
    "    return df.iloc[:split_idx], df.iloc[split_idx:]\n",
    "\n",
    "# Split only the full dataset\n",
    "train_full, test_full = time_series_split(agg_data)\n",
    "\n",
    "# Create simplified versions FROM THE SPLIT DATA\n",
    "train_simple = train_full[simplified_cols]\n",
    "test_simple = test_full[simplified_cols]\n",
    "\n",
    "# ====== VERIFICATION ======\n",
    "def print_date_ranges(df, name):\n",
    "    min_date = f\"{df['date'].min().year}-{df['date'].min().month:02d}\"\n",
    "    max_date = f\"{df['date'].max().year}-{df['date'].max().month:02d}\"\n",
    "    print(f\"{name}: {min_date} to {max_date} ({len(df)} months)\")\n",
    "\n",
    "print(\"\\nDate Range Verification:\")\n",
    "print_date_ranges(train_full, \"Train Full\")\n",
    "print_date_ranges(test_full, \"Test Full\") \n",
    "\n",
    "# ====== SAVING ======\n",
    "datasets = {\n",
    "    'train_full.csv': train_full,\n",
    "    'test_full.csv': test_full,\n",
    "    'train_simple.csv': train_simple,\n",
    "    'test_simple.csv': test_simple\n",
    "}\n",
    "\n",
    "for filename, data in datasets.items():\n",
    "    data = data.drop(columns=['date'])\n",
    "    data.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "print(\"\\nAll data processed and split successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
